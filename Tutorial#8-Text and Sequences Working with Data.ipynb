{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWVYhx8E9AgL"
      },
      "source": [
        "# Text and Sequences Working with Data\n",
        "\n",
        "Our tutorial today contains an introduction to word embeddings. You will train your own word embeddings using a simple Keras model for a sentiment classification task, and then visualize them using two methods.\n",
        "\n",
        "\n",
        "## Representing text/sequnces as numbers\n",
        "\n",
        "Machine learning models take vectors (arrays of numbers) as input. When working with text, the first thing you must do is come up with a strategy to convert strings to numbers (or to \"vectorize\" the text) before feeding it to the model. In this section, you will look at three strategies for doing so.\n",
        "\n",
        "### One-hot encodings\n",
        "\n",
        "As a first idea, you might \"one-hot\" encode each word in your vocabulary. Consider the sentence \"The cat sat on the mat\". The vocabulary (or unique words) in this sentence is (cat, mat, on, sat, the). To represent each word, you will create a zero vector with length equal to the vocabulary, then place a one in the index that corresponds to the word. This approach is shown in the following diagram.\n",
        "\n",
        "\n",
        "<img src=\"https://www.tensorflow.org/text/guide/images/one-hot.png\" alt=\"Diagram of one-hot encodings\" width=\"400\" />\n",
        "\n",
        "To create a vector that contains the encoding of the sentence, you could then concatenate the one-hot vectors for each word.\n",
        "\n",
        "Key point: This approach is inefficient. A one-hot encoded vector is sparse (meaning, most indices are zero). Imagine you have 10,000 words in the vocabulary. To one-hot encode each word, you would create a vector where 99.99% of the elements are zero.\n",
        "\n",
        "### Encode each word with a unique number\n",
        "\n",
        "A second approach you might try is to encode each word using a unique number. Continuing the example above, you could assign 1 to \"cat\", 2 to \"mat\", and so on. You could then encode the sentence \"The cat sat on the mat\" as a dense vector like [5, 1, 4, 3, 5, 2]. This approach is efficient. Instead of a sparse vector, you now have a dense one (where all elements are full).\n",
        "\n",
        "There are two downsides to this approach, however:\n",
        "\n",
        "* The integer-encoding is arbitrary (it does not capture any relationship between words).\n",
        "\n",
        "* An integer-encoding can be challenging for a model to interpret. A linear classifier, for example, learns a single weight for each feature. Because there is no relationship between the similarity of any two words and the similarity of their encodings, this feature-weight combination is not meaningful.\n",
        "\n",
        "### Word embeddings\n",
        "\n",
        "Word embeddings give us a way to use an efficient, dense representation in which similar words have a similar encoding. Importantly, you do not have to specify this encoding by hand. An embedding is a dense vector of floating point values (the length of the vector is a parameter you specify). Instead of specifying the values for the embedding manually, they are trainable parameters (weights learned by the model during training, in the same way a model learns weights for a dense layer). It is common to see word embeddings that are 8-dimensional (for small datasets), up to 1024-dimensions when working with large datasets. A higher dimensional embedding can capture fine-grained relationships between words, but takes more data to learn.\n",
        "\n",
        "<img src=\"https://www.tensorflow.org/text/guide/images/embedding2.png\" alt=\"Diagram of one-hot encodings\" width=\"400\" />\n",
        "\n",
        "\n",
        "Above is a diagram for a word embedding. Each word is represented as a 4-dimensional vector of floating point values. Another way to think of an embedding is as \"lookup table\". After these weights have been learned, you can encode each word by looking up the dense vector it corresponds to in the table.\n",
        "\n",
        "Let's get started with an example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "AEQJPLeJ9AgM"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "import os\n",
        "import re\n",
        "import shutil\n",
        "import string\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yeR3p9yQ9AgN"
      },
      "source": [
        "### Download the IMDb Dataset\n",
        "You will use the [Large Movie Review Dataset](http://ai.stanford.edu/~amaas/data/sentiment/) through the tutorial. You will train a sentiment classifier model on this dataset and in the process learn embeddings from scratch. To read more about loading a dataset from scratch, see the [Loading text tutorial](https://www.tensorflow.org/tutorials/load_data/text).  \n",
        "\n",
        "Download the dataset using Keras file utility and take a look at the directories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cm1k1PXE9AgN",
        "outputId": "8aaa004a-d401-49e8-e0af-7ab5116deb6d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['README', 'test', 'imdb.vocab', 'imdbEr.txt', 'train']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "#Download the dataset using Keras file utility and take a look at the directories\n",
        "url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
        "\n",
        "dataset = tf.keras.utils.get_file(\"aclImdb_v1.tar.gz\", url,\n",
        "                                  untar=True, cache_dir='.',\n",
        "                                  cache_subdir='')\n",
        "\n",
        "dataset_dir = os.path.join(os.path.dirname(dataset), 'aclImdb')\n",
        "os.listdir(dataset_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Up065Y3g9AgO"
      },
      "source": [
        "Take a look at the train/ directory. It has pos and neg folders with movie reviews labelled as positive and negative respectively. You will use reviews from pos and neg folders to train a binary classification model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WH1AKwao9AgO",
        "outputId": "13bbe4f5-2ed7-42c7-9e76-bc2313439905"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['urls_pos.txt',\n",
              " 'unsup',\n",
              " 'labeledBow.feat',\n",
              " 'urls_neg.txt',\n",
              " 'neg',\n",
              " 'urls_unsup.txt',\n",
              " 'pos',\n",
              " 'unsupBow.feat']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "train_dir = os.path.join(dataset_dir, 'train')\n",
        "os.listdir(train_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMWD4mA_9AgP"
      },
      "source": [
        "The train directory also has additional folders which should be removed before creating training dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Q9Qp95Mx9AgP"
      },
      "outputs": [],
      "source": [
        "remove_dir = os.path.join(train_dir, 'unsup')\n",
        "shutil.rmtree(remove_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVj-U7Ej9AgP"
      },
      "source": [
        "Next, we create a tf.data.Dataset using tf.keras.preprocessing.text_dataset_from_directory\n",
        "\n",
        "We will the train directory to create training and validation datasets with a split of 20% for validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I917-Aci9AgP",
        "outputId": "f0c01a79-5be2-45d2-f1b9-070cac7d4005"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 25000 files belonging to 2 classes.\n",
            "Using 20000 files for training.\n",
            "Found 25000 files belonging to 2 classes.\n",
            "Using 5000 files for validation.\n"
          ]
        }
      ],
      "source": [
        "batch_size = 1024\n",
        "seed = 123\n",
        "train_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    'aclImdb/train', batch_size=batch_size, validation_split=0.2,\n",
        "    subset='training', seed=seed)\n",
        "val_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    'aclImdb/train', batch_size=batch_size, validation_split=0.2,\n",
        "    subset='validation', seed=seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNZ7im8a9AgP"
      },
      "source": [
        "Take a look at a few movie reviews and their labels (1: positive, 0: negative) from the train dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5ucBWCo9AgQ",
        "outputId": "66685009-71ae-4f7c-89c9-d041763caecc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 b\"Oh My God! Please, for the love of all that is holy, Do Not Watch This Movie! It it 82 minutes of my life I will never get back. Sure, I could have stopped watching half way through. But I thought it might get better. It Didn't. Anyone who actually enjoyed this movie is one seriously sick and twisted individual. No wonder us Australians/New Zealanders have a terrible reputation when it comes to making movies. Everything about this movie is horrible, from the acting to the editing. I don't even normally write reviews on here, but in this case I'll make an exception. I only wish someone had of warned me before I hired this catastrophe\"\n",
            "1 b'This movie is SOOOO funny!!! The acting is WONDERFUL, the Ramones are sexy, the jokes are subtle, and the plot is just what every high schooler dreams of doing to his/her school. I absolutely loved the soundtrack as well as the carefully placed cynicism. If you like monty python, You will love this film. This movie is a tad bit \"grease\"esk (without all the annoying songs). The songs that are sung are likable; you might even find yourself singing these songs once the movie is through. This musical ranks number two in musicals to me (second next to the blues brothers). But please, do not think of it as a musical per say; seeing as how the songs are so likable, it is hard to tell a carefully choreographed scene is taking place. I think of this movie as more of a comedy with undertones of romance. You will be reminded of what it was like to be a rebellious teenager; needless to say, you will be reminiscing of your old high school days after seeing this film. Highly recommended for both the family (since it is a very youthful but also for adults since there are many jokes that are funnier with age and experience.'\n",
            "0 b\"Alex D. Linz replaces Macaulay Culkin as the central figure in the third movie in the Home Alone empire. Four industrial spies acquire a missile guidance system computer chip and smuggle it through an airport inside a remote controlled toy car. Because of baggage confusion, grouchy Mrs. Hess (Marian Seldes) gets the car. She gives it to her neighbor, Alex (Linz), just before the spies turn up. The spies rent a house in order to burglarize each house in the neighborhood until they locate the car. Home alone with the chicken pox, Alex calls 911 each time he spots a theft in progress, but the spies always manage to elude the police while Alex is accused of making prank calls. The spies finally turn their attentions toward Alex, unaware that he has rigged devices to cleverly booby-trap his entire house. Home Alone 3 wasn't horrible, but probably shouldn't have been made, you can't just replace Macauley Culkin, Joe Pesci, or Daniel Stern. Home Alone 3 had some funny parts, but I don't like when characters are changed in a movie series, view at own risk.\"\n"
          ]
        }
      ],
      "source": [
        "for text_batch, label_batch in train_ds.take(1):\n",
        "  for i in range(3):\n",
        "    print(label_batch[i].numpy(), text_batch.numpy()[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "617_15JG9AgQ"
      },
      "source": [
        "### Configure the dataset for performance\n",
        "\n",
        "These are two important methods you should use when loading data to make sure that I/O does not become blocking.\n",
        "\n",
        "`.cache()` keeps data in memory after it's loaded off disk. This will ensure the dataset does not become a bottleneck while training your model. If your dataset is too large to fit into memory, you can also use this method to create a performant on-disk cache, which is more efficient to read than many small files.\n",
        "\n",
        "`.prefetch()` overlaps data preprocessing and model execution while training.\n",
        "\n",
        "You can learn more about both methods, as well as how to cache data to disk in the [data performance guide](https://www.tensorflow.org/guide/data_performance)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "8nUUU6Z19AgQ"
      },
      "outputs": [],
      "source": [
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abL8GDrL9AgQ"
      },
      "source": [
        "## Using the Embedding layer\n",
        "\n",
        "Keras makes it easy to use word embeddings. Take a look at the [Embedding](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding) layer.\n",
        "\n",
        "The Embedding layer can be understood as a lookup table that maps from integer indices (which stand for specific words) to dense vectors (their embeddings). The dimensionality (or width) of the embedding is a parameter you can experiment with to see what works well for your problem, much in the same way you would experiment with the number of neurons in a Dense layer.\n",
        "\n",
        "\n",
        "\n",
        "**Embedding layer**\n",
        "\n",
        "Turns positive integers (indexes) into dense vectors of fixed size.\n",
        "\n",
        "e.g. [[4], [20]] -> [[0.25, 0.1], [0.6, -0.2]]\n",
        "\n",
        "This layer can only be used as the first layer in a model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "WWVmgP7O9AgQ"
      },
      "outputs": [],
      "source": [
        "# Embed a 1,000 word vocabulary into 5 dimensions.\n",
        "embedding_layer = tf.keras.layers.Embedding(1000, 5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTrUwtQn9AgQ"
      },
      "source": [
        "When you create an Embedding layer, the weights for the embedding are randomly initialized (just like any other layer). During training, they are gradually adjusted via backpropagation. Once trained, the learned word embeddings will roughly encode similarities between words (as they were learned for the specific problem your model is trained on).\n",
        "\n",
        "If you pass an integer to an embedding layer, the result replaces each integer with the vector from the embedding table:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WXDLVW8e9AgQ",
        "outputId": "4c78afed-46e3-4dd1-9623-40175078d9af"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.03986746, -0.02603028,  0.00135169, -0.03810557, -0.00315285],\n",
              "       [-0.00751262,  0.0499424 ,  0.03021679, -0.04647391,  0.03721069],\n",
              "       [-0.02745832,  0.03979154, -0.03207698,  0.02737275, -0.01746972]],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "result = embedding_layer(tf.constant([1, 2, 3]))\n",
        "result.numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1Z2J6r49AgR"
      },
      "source": [
        "For text or sequence problems, the Embedding layer takes a 2D tensor of integers, of shape `(samples, sequence_length)`, where each entry is a sequence of integers. It can embed sequences of variable lengths. You could feed into the embedding layer above batches with shapes `(32, 10)` (batch of 32 sequences of length 10) or `(64, 15)` (batch of 64 sequences of length 15).\n",
        "\n",
        "The returned tensor has one more axis than the input, the embedding vectors are aligned along the new last axis. Pass it a `(2, 3)` input batch and the output is `(2, 3, N)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lvwq4nE79AgR",
        "outputId": "bf5795d3-fce4-4035-d867-4767a4fbeb27"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([2, 3, 5])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "result = embedding_layer(tf.constant([[0, 1, 2], [3, 4, 5]]))\n",
        "result.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "When given a batch of sequences as input, an embedding layer returns a 3D floating point tensor, of shape `(samples, sequence_length, embedding_dimensionality)`. To convert from this sequence of variable length to a fixed representation there are a variety of standard approaches. You could use an RNN, Attention, or pooling layer before passing it to a Dense layer. This tutorial uses pooling because it's the simplest. The [Text Classification with an RNN](https://www.tensorflow.org/text/tutorials/text_classification_rnn) tutorial is a good next step."
      ],
      "metadata": {
        "id": "kuosWA_ZPFqr"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-VjLyXb9AgR"
      },
      "source": [
        "Next, define the dataset preprocessing steps required for your sentiment classification model.\n",
        "\n",
        "Initialize a TextVectorization layer with the desired parameters to vectorize movie reviews\n",
        "\n",
        "\n",
        "**TextVectorization**\n",
        "\n",
        "This layer has basic options for managing text in a Keras model. It\n",
        "  transforms a batch of strings (one sample = one string) into either a list of\n",
        "  token indices (one sample = 1D tensor of integer token indices) or a dense\n",
        "  representation (one sample = 1D tensor of float values representing data about\n",
        "  the sample's tokens).\n",
        "  \n",
        "  \n",
        "  If desired, the user can call this layer's adapt() method on a dataset.\n",
        "  When this layer is adapted, it will analyze the dataset, determine the\n",
        "  frequency of individual string values, and create a 'vocabulary' from them.\n",
        "  This vocabulary can have unlimited size or be capped, depending on the\n",
        "  configuration options for this layer; if there are more unique values in the\n",
        "  input than the maximum vocabulary size, the most frequent terms will be used\n",
        "  to create the vocabulary.\n",
        "  \n",
        "  \n",
        "  The processing of each sample contains the following steps:\n",
        "  \n",
        "    \n",
        "   1. standardize each sample (usually lowercasing + punctuation stripping)\n",
        "   \n",
        "   2. split each sample into substrings (usually words)\n",
        "   \n",
        "   3. recombine substrings into tokens (usually ngrams)\n",
        "    \n",
        "   4. index tokens (associate a unique int value with each token)\n",
        "    \n",
        "   5. transform each sample using this index, either into a vector of ints or\n",
        "       a dense float vector.\n",
        "       \n",
        "  Some notes on passing Callables to customize splitting and normalization for\n",
        "  this layer:\n",
        "   \n",
        "   1. Any callable can be passed to this Layer, but if you want to serialize\n",
        "       this object you should only pass functions that are registered Keras\n",
        "       serializables (see `tf.keras.utils.register_keras_serializable` for more\n",
        "       details).\n",
        "   \n",
        "   2. When using a custom callable for `standardize`, the data received\n",
        "       by the callable will be exactly as passed to this layer. The callable\n",
        "       should return a tensor of the same shape as the input.\n",
        "   \n",
        "   3. When using a custom callable for `split`, the data received by the\n",
        "       callable will have the 1st dimension squeezed out - instead of\n",
        "       `[[\"string to split\"], [\"another string to split\"]]`, the Callable will\n",
        "       see `[\"string to split\", \"another string to split\"]`. The callable should\n",
        "       return a Tensor with the first dimension containing the split tokens -\n",
        "       in this example, we should see something like `[[\"string\", \"to\",\n",
        "       \"split\"], [\"another\", \"string\", \"to\", \"split\"]]`. This makes the callable\n",
        "       site natively compatible with `tf.strings.split()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "psIeOSnS9AgR"
      },
      "outputs": [],
      "source": [
        "# Create a custom standardization function to strip HTML break tags '<br />'.\n",
        "def custom_standardization(input_data):\n",
        "  lowercase = tf.strings.lower(input_data)\n",
        "  stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')\n",
        "  return tf.strings.regex_replace(stripped_html,\n",
        "                                  '[%s]' % re.escape(string.punctuation), '')\n",
        "\n",
        "\n",
        "# Vocabulary size and number of words in a sequence.\n",
        "vocab_size = 10000\n",
        "sequence_length = 100\n",
        "\n",
        "# Use the text vectorization layer to normalize, split, and map strings to\n",
        "# integers. Note that the layer uses the custom standardization defined above.\n",
        "# Set maximum_sequence length as all samples are not of the same length.\n",
        "vectorize_layer = TextVectorization(\n",
        "    standardize=custom_standardization,\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=sequence_length)\n",
        "\n",
        "# Make a text-only dataset (no labels) and call adapt to build the vocabulary.\n",
        "text_ds = train_ds.map(lambda x, y: x)\n",
        "vectorize_layer.adapt(text_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPYRYxX69AgR"
      },
      "source": [
        "## Create a classification model\n",
        "\n",
        "Use the [Keras Sequential API](https://www.tensorflow.org/guide/keras/sequential_model) to define the sentiment classification model. In this case it is a \"Continuous bag of words\" style model.\n",
        "* The [`TextVectorization`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/TextVectorization) layer transforms strings into vocabulary indices. You have already initialized `vectorize_layer` as a TextVectorization layer and built its vocabulary by calling `adapt` on `text_ds`. Now vectorize_layer can be used as the first layer of your end-to-end classification model, feeding transformed strings into the Embedding layer.\n",
        "* The [`Embedding`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding) layer takes the integer-encoded vocabulary and looks up the embedding vector for each word-index. These vectors are learned as the model trains. The vectors add a dimension to the output array. The resulting dimensions are: `(batch, sequence, embedding)`.\n",
        "\n",
        "* The [`GlobalAveragePooling1D`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/GlobalAveragePooling1D) layer returns a fixed-length output vector for each example by averaging over the sequence dimension. This allows the model to handle input of variable length, in the simplest way possible.\n",
        "\n",
        "* The fixed-length output vector is piped through a fully-connected ([`Dense`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense)) layer with 16 hidden units.\n",
        "\n",
        "* The last layer is densely connected with a single output node.\n",
        "\n",
        "Caution: This model doesn't use masking, so the zero-padding is used as part of the input and hence the padding length may affect the output.  To fix this, see the [masking and padding guide](https://www.tensorflow.org/guide/keras/masking_and_padding)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "MsgfD3dM9AgR"
      },
      "outputs": [],
      "source": [
        "embedding_dim=16\n",
        "\n",
        "model = Sequential([\n",
        "  vectorize_layer,\n",
        "  Embedding(vocab_size, embedding_dim, name=\"embedding\"),\n",
        "  GlobalAveragePooling1D(),\n",
        "  Dense(16, activation='relu'),\n",
        "  Dense(1)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "uN6qmYzT9AgR"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "__o8d2bw9AgS",
        "outputId": "05495aa1-66f9-4dac-b979-d769d71b239b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "20/20 [==============================] - 9s 300ms/step - loss: 0.6916 - accuracy: 0.5028 - val_loss: 0.6896 - val_accuracy: 0.4886\n",
            "Epoch 2/15\n",
            "20/20 [==============================] - 1s 68ms/step - loss: 0.6856 - accuracy: 0.5028 - val_loss: 0.6818 - val_accuracy: 0.4886\n",
            "Epoch 3/15\n",
            "20/20 [==============================] - 1s 73ms/step - loss: 0.6750 - accuracy: 0.5028 - val_loss: 0.6693 - val_accuracy: 0.4886\n",
            "Epoch 4/15\n",
            "20/20 [==============================] - 2s 102ms/step - loss: 0.6586 - accuracy: 0.5028 - val_loss: 0.6511 - val_accuracy: 0.4886\n",
            "Epoch 5/15\n",
            "20/20 [==============================] - 1s 71ms/step - loss: 0.6358 - accuracy: 0.5028 - val_loss: 0.6275 - val_accuracy: 0.4886\n",
            "Epoch 6/15\n",
            "20/20 [==============================] - 1s 69ms/step - loss: 0.6073 - accuracy: 0.5059 - val_loss: 0.5997 - val_accuracy: 0.5112\n",
            "Epoch 7/15\n",
            "20/20 [==============================] - 1s 71ms/step - loss: 0.5742 - accuracy: 0.5908 - val_loss: 0.5692 - val_accuracy: 0.6102\n",
            "Epoch 8/15\n",
            "20/20 [==============================] - 1s 68ms/step - loss: 0.5388 - accuracy: 0.6758 - val_loss: 0.5389 - val_accuracy: 0.6610\n",
            "Epoch 9/15\n",
            "20/20 [==============================] - 1s 70ms/step - loss: 0.5039 - accuracy: 0.7288 - val_loss: 0.5107 - val_accuracy: 0.7026\n",
            "Epoch 10/15\n",
            "20/20 [==============================] - 1s 72ms/step - loss: 0.4712 - accuracy: 0.7634 - val_loss: 0.4856 - val_accuracy: 0.7328\n",
            "Epoch 11/15\n",
            "20/20 [==============================] - 2s 103ms/step - loss: 0.4413 - accuracy: 0.7908 - val_loss: 0.4641 - val_accuracy: 0.7526\n",
            "Epoch 12/15\n",
            "20/20 [==============================] - 1s 68ms/step - loss: 0.4147 - accuracy: 0.8112 - val_loss: 0.4459 - val_accuracy: 0.7642\n",
            "Epoch 13/15\n",
            "20/20 [==============================] - 1s 67ms/step - loss: 0.3910 - accuracy: 0.8274 - val_loss: 0.4309 - val_accuracy: 0.7760\n",
            "Epoch 14/15\n",
            "20/20 [==============================] - 1s 69ms/step - loss: 0.3701 - accuracy: 0.8393 - val_loss: 0.4186 - val_accuracy: 0.7836\n",
            "Epoch 15/15\n",
            "20/20 [==============================] - 1s 70ms/step - loss: 0.3515 - accuracy: 0.8500 - val_loss: 0.4086 - val_accuracy: 0.7930\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7dd815dcde10>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v5SLPwFZ9AgS",
        "outputId": "060941df-0bcf-4262-9efb-ab4daea3e6b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " text_vectorization (TextVe  (None, 100)               0         \n",
            " ctorization)                                                    \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, 100, 16)           160000    \n",
            "                                                                 \n",
            " global_average_pooling1d (  (None, 16)                0         \n",
            " GlobalAveragePooling1D)                                         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 16)                272       \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 160289 (626.13 KB)\n",
            "Trainable params: 160289 (626.13 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twdRkCtx9AgS"
      },
      "source": [
        "## Retrieve the trained word embeddings and save them to disk\n",
        "\n",
        "Next, retrieve the word embeddings learned during training. The embeddings are weights of the Embedding layer in the model. The weights matrix is of shape `(vocab_size, embedding_dimension)`.\n",
        "\n",
        "Obtain the weights from the model using `get_layer()` and `get_weights()`. The `get_vocabulary()` function provides the vocabulary to build a metadata file with one token per line."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "65YToiqh9AgS"
      },
      "outputs": [],
      "source": [
        "weights = model.get_layer('embedding').get_weights()[0]\n",
        "vocab = vectorize_layer.get_vocabulary()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write the weights to disk. To use the [Embedding Projector](http://projector.tensorflow.org), you will upload two files in tab separated format: a file of vectors (containing the embedding), and a file of meta data (containing the words)."
      ],
      "metadata": {
        "id": "raYipkidP1Up"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "i0L33NJ89AgS"
      },
      "outputs": [],
      "source": [
        "out_v = io.open('vectors.tsv', 'w', encoding='utf-8')\n",
        "out_m = io.open('metadata.tsv', 'w', encoding='utf-8')\n",
        "\n",
        "for index, word in enumerate(vocab):\n",
        "  if index == 0:\n",
        "    continue  # skip 0, it's padding.\n",
        "  vec = weights[index]\n",
        "  out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
        "  out_m.write(word + \"\\n\")\n",
        "out_v.close()\n",
        "out_m.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptLFvo4d9AgS"
      },
      "source": [
        "## Bag of Words (BoW)\n",
        "\n",
        "A bag-of-words is a representation of text that describes the occurrence of words within a document. It involves two things:\n",
        "\n",
        "1. A vocabulary of known words.\n",
        "2. A measure of the presence of known words.\n",
        "\n",
        "### Step 1: Collect Data\n",
        "\n",
        "    It was the best of times,\n",
        "    it was the worst of times,\n",
        "    it was the age of wisdom,\n",
        "    it was the age of foolishness,\n",
        "\n",
        "For this small example, let’s treat each line as a separate “document” and the 4 lines as our entire corpus of documents.\n",
        "\n",
        "### Step 2: Design the Vocabulary\n",
        "Now we can make a list of all of the words in our model vocabulary.\n",
        "\n",
        "The unique words here (ignoring case and punctuation) are:\n",
        "\n",
        "“it”\n",
        "\n",
        "“was”\n",
        "\n",
        "“the”\n",
        "\n",
        "“best”\n",
        "\n",
        "“of”\n",
        "\n",
        "“times”\n",
        "\n",
        "“worst”\n",
        "\n",
        "“age”\n",
        "\n",
        "“wisdom”\n",
        "\n",
        "“foolishness”\n",
        "\n",
        "That is a vocabulary of 10 words from a corpus containing 24 words.\n",
        "\n",
        "### Step 3: Create Document Vectors\n",
        "The next step is to score the words in each document.\n",
        "\n",
        "The objective is to turn each document of free text into a vector that we can use as input or output for a machine learning model.\n",
        "\n",
        "Because we know the vocabulary has 10 words, we can use a fixed-length document representation of 10, with one position in the vector to score each word.\n",
        "\n",
        "The simplest scoring method is to mark the presence of words as a boolean value, 0 for absent, 1 for present.\n",
        "\n",
        "Using the arbitrary ordering of words listed above in our vocabulary, we can step through the first document (“It was the best of times“) and convert it into a binary vector.\n",
        "\n",
        "The scoring of the document would look as follows:\n",
        "\n",
        "“it” = 1\n",
        "\n",
        "“was” = 1\n",
        "\n",
        "“the” = 1\n",
        "\n",
        "“best” = 1\n",
        "\n",
        "“of” = 1\n",
        "\n",
        "“times” = 1\n",
        "\n",
        "“worst” = 0\n",
        "\n",
        "“age” = 0\n",
        "\n",
        "“wisdom” = 0\n",
        "\n",
        "“foolishness” = 0\n",
        "\n",
        "As a binary vector, this would look as follows:\n",
        "[1, 1, 1, 1, 1, 1, 0, 0, 0, 0]\n",
        "\n",
        "The other three documents would look as follows:\n",
        "\n",
        "\"it was the worst of times\" = [1, 1, 1, 0, 1, 1, 1, 0, 0, 0]\n",
        "\n",
        "\"it was the age of wisdom\" = [1, 1, 1, 0, 1, 0, 0, 1, 1, 0]\n",
        "\n",
        "\"it was the age of foolishness\" = [1, 1, 1, 0, 1, 0, 0, 1, 0, 1]\n",
        "\n",
        "\n",
        "#### Practice what words are most likely to appear in a spam email?\n",
        "\n",
        "### Limitations\n",
        "\n",
        "- **Vocabulary**: How many words in English?\n",
        "\n",
        "- **Sparsity**: (0,1) hard to model.\n",
        "\n",
        "- **Meaning**: Discarding word order ignores the context, and in turn meaning of words in the document (semantics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDLVk1bh9AgT"
      },
      "source": [
        "## Word2Vec Algorithem\n",
        "Word2Vec is not a singular algorithm, rather, it is a family of model architectures and optimizations that can be used to learn word embeddings from large datasets. Embeddings learned through Word2Vec have proven to be successful on a variety of downstream natural language processing tasks.\n",
        "\n",
        "Efficient Estimation of Word Representations in\n",
        "Vector Space https://arxiv.org/pdf/1301.3781.pdf\n",
        "\n",
        "Distributed Representations of Words and Phrases\n",
        "and their Compositionality https://papers.nips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf\n",
        "\n",
        "These papers proposed two methods for learning representations of words:\n",
        "\n",
        "- Continuous Bag-of-Words Model which predicts the middle word based on surrounding context words. The context consists of a few words before and after the current (middle) word. This architecture is called a bag-of-words model as the order of words in the context is not important.\n",
        "- Continuous Skip-gram Model which predict words within a certain range before and after the current word in the same sentence. A worked example of this is given below.\n",
        "\n",
        "You'll use the skip-gram approach in this tutorial. First, you'll explore skip-grams and other concepts using a single sentence for illustration. Next, you'll train your own Word2Vec model on a small dataset.\n",
        "\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/1400/1*xD9n3KeWXuenMNL_BpYp6A.png\" alt=\"Diagram of one-hot encodings\" width=\"600\" />\n",
        "\n",
        "source(medium.com)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0WFPd5Z-9AgT"
      },
      "source": [
        "## Skip-gram and Negative Sampling\n",
        "While a bag-of-words model predicts a word given the neighboring context, a skip-gram model predicts the context (or neighbors) of a word, given the word itself. The model is trained on skip-grams, which are n-grams that allow tokens to be skipped (see the diagram below for an example). The context of a word can be represented through a set of skip-gram pairs of (target_word, context_word) where context_word appears in the neighboring context of target_word.\n",
        "\n",
        "\n",
        "Consider the following sentence of 8 words.\n",
        "> The wide road shimmered in the hot sun.\n",
        "\n",
        "The context words for each of the 8 words of this sentence are defined by a window size. The window size determines the span of words on either side of a `target_word` that can be considered `context word`. Take a look at this table of skip-grams for target words based on different window siz\n",
        "\n",
        "Note: For this tutorial, a window size of *n* implies n words on each side with a total window span of 2*n+1 words across a word.\n",
        "\n",
        "![word2vec_skipgrams](https://tensorflow.org/text/tutorials/images/word2vec_skipgram.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIDKDOXX9AgT"
      },
      "source": [
        "The training objective of the skip-gram model is to maximize the probability of predicting context words given the target word. For a sequence of words w1, w2, ... wT, the objective can be written as the average log probability\n",
        "\n",
        "\n",
        "![word2vec_skipgram_objective](https://tensorflow.org/text/tutorials/images/word2vec_skipgram_objective.png)\n",
        "\n",
        "\n",
        "where c is the size of the training context. The basic skip-gram formulation defines this probability using the softmax function.\n",
        "\n",
        "![word2vec_full_softmax](https://tensorflow.org/text/tutorials/images/word2vec_full_softmax.png)\n",
        "\n",
        "\n",
        "where v and v' are target and context vector representations of words and W is vocabulary size.\n",
        "\n",
        "Computing the denominator of this formulation involves performing a full softmax over the entire vocabulary words which is often large (105-107) terms.\n",
        "\n",
        "The Noise Contrastive Estimation loss function is an efficient approximation for a full softmax. With an objective to learn word embeddings instead of modelling the word distribution, NCE loss can be simplified to use negative sampling.\n",
        "\n",
        "The simplified negative sampling objective for a target word is to distinguish the context word from num_ns negative samples drawn from noise distribution Pn(w) of words. More precisely, an efficient approximation of full softmax over the vocabulary is, for a skip-gram pair, to pose the loss for a target word as a classification problem between the context word and num_ns negative samples.\n",
        "\n",
        "A negative sample is defined as a (target_word, context_word) pair such that the context_word does not appear in the window_size neighborhood of the target_word. For the example sentence, these are few potential negative samples (when window_size is 2).\n",
        "\n",
        "\n",
        "(hot, shimmered)\n",
        "(wide, hot)\n",
        "(wide, sun)\n",
        "\n",
        "In the next section, you'll generate skip-grams and negative samples for a single sentence. You'll also learn about subsampling techniques and train a classification model for positive and negative training examples later in the tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "XLBQEVYr9AgT"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "import re\n",
        "import string\n",
        "import tensorflow as tf\n",
        "import tqdm\n",
        "import numpy as np\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Dot, Embedding, Flatten\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Xzx_lb7f9Agb"
      },
      "outputs": [],
      "source": [
        "SEED = 42\n",
        "AUTOTUNE = tf.data.AUTOTUNE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1g6UvMUI9Agb"
      },
      "source": [
        "### vectorize an example sentence\n",
        "Consider the following sentence:\n",
        "The wide road shimmered in the hot sun.\n",
        "\n",
        "Tokenize the sentence:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C5AVX2WB9Agb",
        "outputId": "909d67a9-4244-4bd9-ce1b-abba3c478e8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8\n"
          ]
        }
      ],
      "source": [
        "sentence = \"The wide road shimmered in the hot sun\"\n",
        "tokens = list(sentence.lower().split())\n",
        "print(len(tokens))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aFT_C9M69Agb",
        "outputId": "696c4e78-01ac-4f88-c84c-599c921996ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'<pad>': 0, 'the': 1, 'wide': 2, 'road': 3, 'shimmered': 4, 'in': 5, 'hot': 6, 'sun': 7}\n"
          ]
        }
      ],
      "source": [
        "# Create a vocabulary to save mappings from tokens to integer indices.\n",
        "\n",
        "vocab, index = {}, 1  # start indexing from 1\n",
        "vocab['<pad>'] = 0  # add a padding token\n",
        "for token in tokens:\n",
        "  if token not in vocab:\n",
        "    vocab[token] = index\n",
        "    index += 1\n",
        "vocab_size = len(vocab)\n",
        "print(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tlwj5Pgw9Agc",
        "outputId": "535b6b02-00c1-4700-9660-d966a8a2f36d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: '<pad>', 1: 'the', 2: 'wide', 3: 'road', 4: 'shimmered', 5: 'in', 6: 'hot', 7: 'sun'}\n"
          ]
        }
      ],
      "source": [
        "# Create an inverse vocabulary to save mappings from integer indices to tokens\n",
        "\n",
        "inverse_vocab = {index: token for token, index in vocab.items()}\n",
        "print(inverse_vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HEyeiV0H9Agc",
        "outputId": "a30380b0-b145-4fc6-a255-059dc23bf776"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 2, 3, 4, 5, 1, 6, 7]\n"
          ]
        }
      ],
      "source": [
        "# Vectorize your sentence\n",
        "example_sequence = [vocab[word] for word in tokens]\n",
        "print(example_sequence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3QAceJb9Agc"
      },
      "source": [
        "### Generating skip-grams from one sentence\n",
        "\n",
        "The `tf.keras.preprocessing.sequence` module provides useful functions that simplify data preparation for word2vec. You can use the `tf.keras.preprocessing.sequence.skipgrams` to generate skip-gram pairs from the `example_sequence` with a given `window_size` from tokens in the range `[0, vocab_size)`.\n",
        "\n",
        "Note: `negative_samples` is set to `0` here, as batching negative samples generated by this function requires a bit of code. You will use another function to perform negative sampling in the next section."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YCfkFsEF9Agc",
        "outputId": "6e76b1db-9523-450e-e375-f9b28f529110"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "26\n"
          ]
        }
      ],
      "source": [
        "window_size = 2\n",
        "positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
        "      example_sequence,\n",
        "      vocabulary_size=vocab_size,\n",
        "      window_size=window_size,\n",
        "      negative_samples=0)\n",
        "print(len(positive_skip_grams))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8lwpaanA9Agc",
        "outputId": "c1b108b9-0f94-4e65-f679-65fa72c25995"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5, 4): (in, shimmered)\n",
            "(1, 4): (the, shimmered)\n",
            "(3, 4): (road, shimmered)\n",
            "(6, 5): (hot, in)\n",
            "(4, 2): (shimmered, wide)\n"
          ]
        }
      ],
      "source": [
        "# Take a look at few positive skip-grams.\n",
        "\n",
        "for target, context in positive_skip_grams[:5]:\n",
        "  print(f\"({target}, {context}): ({inverse_vocab[target]}, {inverse_vocab[context]})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVUDngcw9Agc"
      },
      "source": [
        "### Negative sampling for one skip-gram\n",
        "\n",
        "The `skipgrams` function returns all positive skip-gram pairs by sliding over a given window span. To produce additional skip-gram pairs that would serve as negative samples for training, you need to sample random words from the vocabulary. Use the `tf.random.log_uniform_candidate_sampler` function to sample `num_ns` number of negative samples for a given target word in a window. You can call the function on one skip-grams's target word and pass the context word as true class to exclude it from being sampled.\n",
        "\n",
        "Key point: `num_ns` (the number of negative samples per a positive context word) in the `[5, 20]` range is [shown to work](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf) best for smaller datasets, while `num_ns` in the `[2, 5]` range suffices for larger datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-A6-8mKB9Agc",
        "outputId": "cad66323-1d2f-4ed1-9b71-a4179bf07c7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([2 1 4 3], shape=(4,), dtype=int64)\n",
            "['wide', 'the', 'shimmered', 'road']\n"
          ]
        }
      ],
      "source": [
        "# Get target and context words for one positive skip-gram.\n",
        "target_word, context_word = positive_skip_grams[0]\n",
        "\n",
        "# Set the number of negative samples per positive context.\n",
        "num_ns = 4\n",
        "\n",
        "context_class = tf.reshape(tf.constant(context_word, dtype=\"int64\"), (1, 1))\n",
        "negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
        "    true_classes=context_class,  # class that should be sampled as 'positive'\n",
        "    num_true=1,  # each positive skip-gram has 1 positive context class\n",
        "    num_sampled=num_ns,  # number of negative context words to sample\n",
        "    unique=True,  # all the negative samples should be unique\n",
        "    range_max=vocab_size,  # pick index of the samples from [0, vocab_size]\n",
        "    seed=SEED,  # seed for reproducibility\n",
        "    name=\"negative_sampling\"  # name of this operation\n",
        ")\n",
        "print(negative_sampling_candidates)\n",
        "print([inverse_vocab[index.numpy()] for index in negative_sampling_candidates])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_x40Q_h9Agd"
      },
      "source": [
        "### Construct one training example\n",
        "For a given positive `(target_word, context_word)` skip-gram, you now also have `num_ns` negative sampled context words that do not appear in the window size neighborhood of `target_word`. Batch the `1` positive `context_word` and `num_ns` negative context words into one tensor. This produces a set of positive skip-grams (labeled as `1`) and negative samples (labeled as `0`) for each target word."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "N5n79ReH9Agd"
      },
      "outputs": [],
      "source": [
        "# Reduce a dimension so you can use concatenation (in the next step).\n",
        "squeezed_context_class = tf.squeeze(context_class, 1)\n",
        "\n",
        "# Concatenate a positive context word with negative sampled words.\n",
        "context = tf.concat([squeezed_context_class, negative_sampling_candidates], 0)\n",
        "\n",
        "# Label the first context word as `1` (positive) followed by `num_ns` `0`s (negative).\n",
        "label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\n",
        "target = target_word"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWd936jH9Agd"
      },
      "source": [
        "Take a look at the context and the corresponding labels for the target word from the skip-gram example above.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v3TeeVQc9Agd",
        "outputId": "e8262cbc-e997-4718-a3e9-59d707d0d740"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "target_index    : 5\n",
            "target_word     : in\n",
            "context_indices : [4 2 1 4 3]\n",
            "context_words   : ['shimmered', 'wide', 'the', 'shimmered', 'road']\n",
            "label           : [1 0 0 0 0]\n"
          ]
        }
      ],
      "source": [
        "print(f\"target_index    : {target}\")\n",
        "print(f\"target_word     : {inverse_vocab[target_word]}\")\n",
        "print(f\"context_indices : {context}\")\n",
        "print(f\"context_words   : {[inverse_vocab[c.numpy()] for c in context]}\")\n",
        "print(f\"label           : {label}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4XPFIQY9Agd"
      },
      "source": [
        "A tuple of (target, context, label) tensors constitutes one training example for training your skip-gram negative sampling Word2Vec model. Notice that the target is of shape (1,) while the context and label are of shape (1+num_ns,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cT7fGtVn9Agd",
        "outputId": "10e853b4-a65d-4cbc-9296-5c0b841ba179"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "target  : 5\n",
            "context : tf.Tensor([4 2 1 4 3], shape=(5,), dtype=int64)\n",
            "label   : tf.Tensor([1 0 0 0 0], shape=(5,), dtype=int64)\n"
          ]
        }
      ],
      "source": [
        "print(\"target  :\", target)\n",
        "print(\"context :\", context)\n",
        "print(\"label   :\", label)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgIoJ-5x9Agd"
      },
      "source": [
        "### Summary\n",
        "\n",
        "This picture summarizes the procedure of generating training example from a sentence.\n",
        "\n",
        "![word2vec_negative_sampling](https://tensorflow.org/text/tutorials/images/word2vec_negative_sampling.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jtu9wz3s9Agd"
      },
      "source": [
        "### Compile all steps into one function\n",
        "\n",
        "#### Skip-gram Sampling table\n",
        "A large dataset means larger vocabulary with higher number of more frequent words such as stopwords. Training examples obtained from sampling commonly occurring words (such as the, is, on) don't add much useful information for the model to learn from. Mikolov et al. suggest subsampling of frequent words as a helpful practice to improve embedding quality.\n",
        "\n",
        "The `tf.keras.preprocessing.sequence.skipgrams` function accepts a sampling table argument to encode probabilities of sampling any token. You can use the `tf.keras.preprocessing.sequence.make_sampling_table` to  generate a word-frequency rank based probabilistic sampling table and pass it to the `skipgrams` function. Inspect the sampling probabilities for a `vocab_size` of 10."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZj53UMa9Age",
        "outputId": "fc936889-e96a-486f-ad6a-c4155e5d8cca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.00315225 0.00315225 0.00547597 0.00741556 0.00912817 0.01068435\n",
            " 0.01212381 0.01347162 0.01474487 0.0159558 ]\n"
          ]
        }
      ],
      "source": [
        "sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(size=10)\n",
        "print(sampling_table)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`sampling_table[i]` denotes the probability of sampling the i-th most common word in a dataset. The function assumes a [Zipf's distribution](https://en.wikipedia.org/wiki/Zipf%27s_law) of the word frequencies for sampling.\n",
        "\n",
        "Key point: The `tf.random.log_uniform_candidate_sampler` already assumes that the vocabulary frequency follows a log-uniform (Zipf's) distribution. Using these distribution weighted sampling also helps approximate the Noise Contrastive Estimation (NCE) loss with simpler loss functions for training a negative sampling objective."
      ],
      "metadata": {
        "id": "B6QXJ4_aRYz8"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rv381Xt_9Age"
      },
      "source": [
        "### Generate training data\n",
        "\n",
        "Compile all the steps described above into a function that can be called on a list of vectorized sentences obtained from any text dataset. Notice that the sampling table is built before sampling skip-gram word pairs. You will use this function in the later sections."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "dd26lIwi9Age"
      },
      "outputs": [],
      "source": [
        "# Generates skip-gram pairs with negative sampling for a list of sequences\n",
        "# (int-encoded sentences) based on window size, number of negative samples\n",
        "# and vocabulary size.\n",
        "def generate_training_data(sequences, window_size, num_ns, vocab_size, seed):\n",
        "  # Elements of each training example are appended to these lists.\n",
        "  targets, contexts, labels = [], [], []\n",
        "\n",
        "  # Build the sampling table for `vocab_size` tokens.\n",
        "  sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(vocab_size)\n",
        "\n",
        "  # Iterate over all sequences (sentences) in the dataset.\n",
        "  for sequence in tqdm.tqdm(sequences):\n",
        "\n",
        "    # Generate positive skip-gram pairs for a sequence (sentence).\n",
        "    positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
        "          sequence,\n",
        "          vocabulary_size=vocab_size,\n",
        "          sampling_table=sampling_table,\n",
        "          window_size=window_size,\n",
        "          negative_samples=0)\n",
        "\n",
        "    # Iterate over each positive skip-gram pair to produce training examples\n",
        "    # with a positive context word and negative samples.\n",
        "    for target_word, context_word in positive_skip_grams:\n",
        "      context_class = tf.expand_dims(\n",
        "          tf.constant([context_word], dtype=\"int64\"), 1)\n",
        "      negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
        "          true_classes=context_class,\n",
        "          num_true=1,\n",
        "          num_sampled=num_ns,\n",
        "          unique=True,\n",
        "          range_max=vocab_size,\n",
        "          seed=seed,\n",
        "          name=\"negative_sampling\")\n",
        "\n",
        "      # Build context and label vectors (for one target word)\n",
        "      context = tf.concat([tf.squeeze(context_class,1), negative_sampling_candidates], 0)\n",
        "      label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\n",
        "\n",
        "      # Append each element from the training example to global lists.\n",
        "      targets.append(target_word)\n",
        "      contexts.append(context)\n",
        "      labels.append(label)\n",
        "\n",
        "  return targets, contexts, labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Lg1WMS49Age"
      },
      "source": [
        "### Prepare training data for Word2Vec\n",
        "\n",
        "With an understanding of how to work with one sentence for a skip-gram negative sampling based Word2Vec model, you can proceed to generate training examples from a larger list of sentences!\n",
        "\n",
        "#### Download text corpus\n",
        "You will use a text file of Shakespeare's writing for this tutorial. Change the following line to run this code on your own data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "dtunPw5q9Age"
      },
      "outputs": [],
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qFnCIdwv9Age",
        "outputId": "e78b3536-35d8-440d-d40c-f185549276eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n"
          ]
        }
      ],
      "source": [
        "with open(path_to_file) as f:\n",
        "  lines = f.read().splitlines()\n",
        "for line in lines[:20]:\n",
        "  print(line)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "P4eRDPyi9Age"
      },
      "outputs": [],
      "source": [
        "# Use the non empty lines to construct a tf.data.TextLineDataset object for next steps.\n",
        "text_ds = tf.data.TextLineDataset(path_to_file).filter(lambda x: tf.cast(tf.strings.length(x), bool))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCEHRW-W9Agf"
      },
      "source": [
        "Vectorize sentences from the corpus\n",
        "\n",
        "You can use the TextVectorization layer to vectorize sentences from the corpus. Learn more about using this layer in this Text Classification tutorial. Notice from the first few sentences above that the text needs to be in one case and punctuation needs to be removed. To do this, define a custom_standardization function that can be used in the TextVectorization layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "1jwxWy_W9Agf"
      },
      "outputs": [],
      "source": [
        "# Now, create a custom standardization function to lowercase the text and\n",
        "# remove punctuation.\n",
        "def custom_standardization(input_data):\n",
        "  lowercase = tf.strings.lower(input_data)\n",
        "  return tf.strings.regex_replace(lowercase,\n",
        "                                  '[%s]' % re.escape(string.punctuation), '')\n",
        "\n",
        "\n",
        "# Define the vocabulary size and number of words in a sequence.\n",
        "vocab_size = 4096\n",
        "sequence_length = 10\n",
        "\n",
        "# Use the text vectorization layer to normalize, split, and map strings to\n",
        "# integers. Set output_sequence_length length to pad all samples to same length.\n",
        "vectorize_layer = TextVectorization(\n",
        "    standardize=custom_standardization,\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=sequence_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "9QZUdid09Agf"
      },
      "outputs": [],
      "source": [
        "# Call adapt on the text dataset to create vocabulary.\n",
        "vectorize_layer.adapt(text_ds.batch(1024))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEcOp2O-9Agf"
      },
      "source": [
        "Once the state of the layer has been adapted to represent the text corpus, the vocabulary can be accessed with get_vocabulary(). This function returns a list of all vocabulary tokens sorted (descending) by their frequency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BaT44lNN9Agf",
        "outputId": "675f151c-b6e6-466f-9eaa-86de7fa9ed19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['', '[UNK]', 'the', 'and', 'to', 'i', 'of', 'you', 'my', 'a', 'that', 'in', 'is', 'not', 'for', 'with', 'me', 'it', 'be', 'your']\n"
          ]
        }
      ],
      "source": [
        "# Save the created vocabulary for reference.\n",
        "inverse_vocab = vectorize_layer.get_vocabulary()\n",
        "print(inverse_vocab[:20])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPMTCecV9Agf"
      },
      "source": [
        "The vectorize_layer can now be used to generate vectors for each element in the text_ds."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "51Mv7tGx9Agf"
      },
      "outputs": [],
      "source": [
        "# Vectorize the data in text_ds.\n",
        "text_vector_ds = text_ds.batch(1024).prefetch(AUTOTUNE).map(vectorize_layer).unbatch()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kT31CKLG9Agf"
      },
      "source": [
        "### Obtain sequences from the dataset\n",
        "You now have a tf.data.Dataset of integer encoded sentences. To prepare the dataset for training a Word2Vec model, flatten the dataset into a list of sentence vector sequences. This step is required as you would iterate over each sentence in the dataset to produce positive and negative examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VRg8_qpb9Agg",
        "outputId": "b50b5c57-4fd1-4ad1-956d-9b4c2e99b296"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32777\n"
          ]
        }
      ],
      "source": [
        "sequences = list(text_vector_ds.as_numpy_iterator())\n",
        "print(len(sequences))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UceQ3Tv99Agg",
        "outputId": "ac0a17ae-9485-40ff-cdce-b89bb1cd1f96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 89 270   0   0   0   0   0   0   0   0] => ['first', 'citizen', '', '', '', '', '', '', '', '']\n",
            "[138  36 982 144 673 125  16 106   0   0] => ['before', 'we', 'proceed', 'any', 'further', 'hear', 'me', 'speak', '', '']\n",
            "[34  0  0  0  0  0  0  0  0  0] => ['all', '', '', '', '', '', '', '', '', '']\n",
            "[106 106   0   0   0   0   0   0   0   0] => ['speak', 'speak', '', '', '', '', '', '', '', '']\n",
            "[ 89 270   0   0   0   0   0   0   0   0] => ['first', 'citizen', '', '', '', '', '', '', '', '']\n"
          ]
        }
      ],
      "source": [
        "#Take a look at few examples from sequences.\n",
        "for seq in sequences[:5]:\n",
        "  print(f\"{seq} => {[inverse_vocab[i] for i in seq]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-gGpltC79Agg"
      },
      "source": [
        "### Generate training examples from sequences\n",
        "\n",
        "`sequences` is now a list of int encoded sentences. Just call the `generate_training_data` function defined earlier to generate training examples for the word2vec model. To recap, the function iterates over each word from each sequence to collect positive and negative context words. Length of target, contexts and labels should be the same, representing the total number of training examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lUPRT2pa9Agg",
        "outputId": "e79bbb8e-f38a-4b07-ca87-ed50482fde2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 32777/32777 [01:21<00:00, 401.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "targets.shape: (64944,)\n",
            "contexts.shape: (64944, 5)\n",
            "labels.shape: (64944, 5)\n"
          ]
        }
      ],
      "source": [
        "targets, contexts, labels = generate_training_data(\n",
        "    sequences=sequences,\n",
        "    window_size=2,\n",
        "    num_ns=4,\n",
        "    vocab_size=vocab_size,\n",
        "    seed=SEED)\n",
        "\n",
        "targets = np.array(targets)\n",
        "contexts = np.array(contexts)\n",
        "labels = np.array(labels)\n",
        "\n",
        "print('\\n')\n",
        "print(f\"targets.shape: {targets.shape}\")\n",
        "print(f\"contexts.shape: {contexts.shape}\")\n",
        "print(f\"labels.shape: {labels.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dkf5COlW9Agg",
        "outputId": "08c0f589-7fcd-444c-fed3-87c8352cd398"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<_BatchDataset element_spec=((TensorSpec(shape=(1024,), dtype=tf.int64, name=None), TensorSpec(shape=(1024, 5), dtype=tf.int64, name=None)), TensorSpec(shape=(1024, 5), dtype=tf.int64, name=None))>\n"
          ]
        }
      ],
      "source": [
        "BATCH_SIZE = 1024\n",
        "BUFFER_SIZE = 10000\n",
        "dataset = tf.data.Dataset.from_tensor_slices(((targets, contexts), labels))\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "print(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uRLxXr0_9Agg",
        "outputId": "7edb55ff-d8bd-4b34-af25-a1b7972b03f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<_PrefetchDataset element_spec=((TensorSpec(shape=(1024,), dtype=tf.int64, name=None), TensorSpec(shape=(1024, 5), dtype=tf.int64, name=None)), TensorSpec(shape=(1024, 5), dtype=tf.int64, name=None))>\n"
          ]
        }
      ],
      "source": [
        "dataset = dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "print(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qB5TMxVy9Agg"
      },
      "source": [
        "### Model and Training\n",
        "The Word2Vec model can be implemented as a classifier to distinguish between true context words from skip-grams and false context words obtained through negative sampling. You can perform a dot product between the embeddings of target and context words to obtain predictions for labels and compute loss against true labels in the dataset.\n",
        "\n",
        "#### Subclassed Word2Vec Model\n",
        "Use the [Keras Subclassing API](https://www.tensorflow.org/guide/keras/custom_layers_and_models) to define your word2vec model with the following layers:\n",
        "\n",
        "* `target_embedding`: A `tf.keras.layers.Embedding` layer, which looks up the embedding of a word when it appears as a target word. The number of parameters in this layer are `(vocab_size * embedding_dim)`.\n",
        "* `context_embedding`: Another `tf.keras.layers.Embedding` layer, which looks up the embedding of a word when it appears as a context word. The number of parameters in this layer are the same as those in `target_embedding`, i.e. `(vocab_size * embedding_dim)`.\n",
        "* `dots`: A `tf.keras.layers.Dot` layer that computes the dot product of target and context embeddings from a training pair.\n",
        "* `flatten`: A `tf.keras.layers.Flatten` layer to flatten the results of `dots` layer into logits.\n",
        "\n",
        "With the subclassed model, you can define the `call()` function that accepts `(target, context)` pairs which can then be passed into their corresponding embedding layer. Reshape the `context_embedding` to perform a dot product with `target_embedding` and return the flattened result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "-WH-oQOo9Agh"
      },
      "outputs": [],
      "source": [
        "class Word2Vec(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim):\n",
        "    super(Word2Vec, self).__init__()\n",
        "    self.target_embedding = layers.Embedding(vocab_size,\n",
        "                                      embedding_dim,\n",
        "                                      input_length=1,\n",
        "                                      name=\"w2v_embedding\")\n",
        "    self.context_embedding = layers.Embedding(vocab_size,\n",
        "                                       embedding_dim,\n",
        "                                       input_length=num_ns+1)\n",
        "\n",
        "  def call(self, pair):\n",
        "    target, context = pair\n",
        "    # target: (batch, dummy?)  # The dummy axis doesn't exist in TF2.7+\n",
        "    # context: (batch, context)\n",
        "    if len(target.shape) == 2:\n",
        "      target = tf.squeeze(target, axis=1)\n",
        "    # target: (batch,)\n",
        "    word_emb = self.target_embedding(target)\n",
        "    # word_emb: (batch, embed)\n",
        "    context_emb = self.context_embedding(context)\n",
        "    # context_emb: (batch, context, embed)\n",
        "    dots = tf.einsum('be,bce->bc', word_emb, context_emb)\n",
        "    # dots: (batch, context)\n",
        "    return dots"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_kF1aNI9Agh"
      },
      "source": [
        "For simplicity, you can use `tf.keras.losses.CategoricalCrossEntropy` as an alternative to the negative sampling loss. If you would like to write your own custom loss function, you can also do so as follows:\n",
        "\n",
        "``` python\n",
        "def custom_loss(x_logit, y_true):\n",
        "      return tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logit, labels=y_true)\n",
        "```\n",
        "\n",
        "It's time to build your model! Instantiate your word2vec class with an embedding dimension of 128 (you could experiment with different values). Compile the model with the `tf.keras.optimizers.Adam` optimizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "eEIAYUl29Agh"
      },
      "outputs": [],
      "source": [
        "embedding_dim = 128\n",
        "word2vec = Word2Vec(vocab_size, embedding_dim)\n",
        "word2vec.compile(optimizer='adam',\n",
        "                 loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "                 metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aNBnA7B19Agh",
        "outputId": "9977cae0-26d1-4345-c4b2-6eaba403475f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "63/63 [==============================] - 22s 332ms/step - loss: 1.6082 - accuracy: 0.2320\n",
            "Epoch 2/20\n",
            "63/63 [==============================] - 0s 4ms/step - loss: 1.5890 - accuracy: 0.5592\n",
            "Epoch 3/20\n",
            "63/63 [==============================] - 0s 4ms/step - loss: 1.5418 - accuracy: 0.6115\n",
            "Epoch 4/20\n",
            "63/63 [==============================] - 0s 4ms/step - loss: 1.4586 - accuracy: 0.5841\n",
            "Epoch 5/20\n",
            "63/63 [==============================] - 0s 4ms/step - loss: 1.3590 - accuracy: 0.5861\n",
            "Epoch 6/20\n",
            "63/63 [==============================] - 0s 4ms/step - loss: 1.2611 - accuracy: 0.6109\n",
            "Epoch 7/20\n",
            "63/63 [==============================] - 0s 4ms/step - loss: 1.1706 - accuracy: 0.6416\n",
            "Epoch 8/20\n",
            "63/63 [==============================] - 0s 4ms/step - loss: 1.0873 - accuracy: 0.6757\n",
            "Epoch 9/20\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 1.0102 - accuracy: 0.7079\n",
            "Epoch 10/20\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.9385 - accuracy: 0.7376\n",
            "Epoch 11/20\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.8720 - accuracy: 0.7633\n",
            "Epoch 12/20\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.8104 - accuracy: 0.7855\n",
            "Epoch 13/20\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.7535 - accuracy: 0.8052\n",
            "Epoch 14/20\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.7012 - accuracy: 0.8221\n",
            "Epoch 15/20\n",
            "63/63 [==============================] - 0s 4ms/step - loss: 0.6533 - accuracy: 0.8369\n",
            "Epoch 16/20\n",
            "63/63 [==============================] - 0s 4ms/step - loss: 0.6095 - accuracy: 0.8512\n",
            "Epoch 17/20\n",
            "63/63 [==============================] - 0s 4ms/step - loss: 0.5696 - accuracy: 0.8635\n",
            "Epoch 18/20\n",
            "63/63 [==============================] - 0s 4ms/step - loss: 0.5332 - accuracy: 0.8744\n",
            "Epoch 19/20\n",
            "63/63 [==============================] - 0s 4ms/step - loss: 0.5001 - accuracy: 0.8845\n",
            "Epoch 20/20\n",
            "63/63 [==============================] - 0s 4ms/step - loss: 0.4700 - accuracy: 0.8936\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7dd816de61a0>"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ],
      "source": [
        "word2vec.fit(dataset, epochs=20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6Z-FMov9Agi"
      },
      "source": [
        "### Embedding lookup and analysis\n",
        "Obtain the weights from the model using get_layer() and get_weights(). The get_vocabulary() function provides the vocabulary to build a metadata file with one token per line."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "XVBk1Lm59Agi"
      },
      "outputs": [],
      "source": [
        "weights = word2vec.get_layer('w2v_embedding').get_weights()[0]\n",
        "vocab = vectorize_layer.get_vocabulary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "vRALOxw_9Agi"
      },
      "outputs": [],
      "source": [
        "#Create and save the vectors and metadata file.\n",
        "out_v = io.open('vectorsw2v.tsv', 'w', encoding='utf-8')\n",
        "out_m = io.open('metadataw2v.tsv', 'w', encoding='utf-8')\n",
        "\n",
        "for index, word in enumerate(vocab):\n",
        "  if index == 0:\n",
        "    continue  # skip 0, it's padding.\n",
        "  vec = weights[index]\n",
        "  out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
        "  out_m.write(word + \"\\n\")\n",
        "out_v.close()\n",
        "out_m.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FpHhTZ9m9Agi"
      },
      "source": [
        "More on state of the art: http://jalammar.github.io/how-gpt3-works-visualizations-animations/\n",
        "\n",
        "Optional: https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/structured_data/time_series.ipynb\n",
        "\n",
        "References:\n",
        "    - François Chollet\n",
        "    - https://www.tensorflow.org/"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}