{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Neural Networks (GNN)\n",
    "\n",
    "For this part, we will be using spektral to create GNN.\n",
    "\n",
    "## What is spektral?\n",
    "Spektral is a Python library for graph deep learning, based on the Keras API and TensorFlow 2. The main goal of this project is to provide a simple but flexible framework for creating graph neural networks (GNNs).\n",
    "\n",
    "You can use Spektral for classifying the users of a social network, predicting molecular properties, generating new graphs with GANs, clustering nodes, predicting links, and any other task where data is described by graphs.\n",
    "\n",
    "Spektral implements some of the most popular layers for graph deep learning, including:\n",
    "\n",
    "- Graph Convolutional Networks (GCN)\n",
    "- Chebyshev convolutions\n",
    "- GraphSAGE\n",
    "- ARMA convolutions\n",
    "- Edge-Conditioned Convolutions (ECC)\n",
    "- Graph attention networks (GAT)\n",
    "- Approximated Personalized Propagation of Neural Predictions (APPNP)\n",
    "- Graph Isomorphism Networks (GIN)\n",
    "- Diffusional Convolutions\n",
    "\n",
    "\n",
    "### Graphs\n",
    "A graph is a mathematical object that represents relations between entities. We call the entities \"nodes\" and the relations \"edges\".\n",
    "\n",
    "Both the nodes and the edges can have vector features.\n",
    "\n",
    "In Spektral, graphs are represented with instances of spektral.data.Graph. A graph can have four main attributes:\n",
    "\n",
    "a: the adjacency matrix\n",
    "x: the node features\n",
    "e: the edge features\n",
    "y: the labels\n",
    "A graph can have all of these attributes or none of them. Since Graphs are just plain Python objects, you can also add extra attributes if you want. For instance, see graph.n_nodes, graph.n_node_features, etc.\n",
    "\n",
    "### Adjacency matrix (graph.a)\n",
    "Each entry a[i, j] of the adjacency matrix is non-zero if there exists an edge going from node j to node i, and zero otherwise.\n",
    "\n",
    "We can represent a as a dense np.array or as a Scipy sparse matrix of shape [n_nodes, n_nodes]. Using an np.array to represent the adjacency matrix can be expensive, since we need to store a lot of 0s in memory, so sparse matrices are usually preferable.\n",
    "\n",
    "With sparse matrices, we only need to store the non-zero entries of a. In practice, we can implement a sparse matrix by only storing the indices and values of the non-zero entries in a list, and assuming that if a pair of indices is missing from the list then its corresponding value will be 0.\n",
    "This is called the COOrdinate format and it is the format used by TensorFlow to represent sparse tensors.\n",
    "\n",
    "For example, the adjacency matrix of a weighted ring graph with 4 nodes:\n",
    "       \n",
    "       [[0, 1, 0, 2],\n",
    "       [3, 0, 4, 0],\n",
    "       [0, 5, 0, 6],\n",
    "       [7, 0, 8, 0]]\n",
    "             \n",
    "can be represented in COOrdinate format as follows:\n",
    "\n",
    "        R, C, V\n",
    "        0, 1, 1\n",
    "        0, 3, 2\n",
    "        1, 0, 3\n",
    "        1, 2, 4\n",
    "        2, 1, 5\n",
    "        2, 3, 6\n",
    "        3, 0, 7\n",
    "        3, 2, 8\n",
    " \n",
    "where R indicates the \"row\" indices, C the columns, and V the non-zero values a[i, j]. For example, in the second line, we see that there is an edge that goes from node 3 to node 0 with weight 2.\n",
    "\n",
    "We also see that, in this case, all edges have a corresponding edge that goes in the opposite direction. For the sake of this example, all edges have been assigned a different weight. In practice, however, edge i, j will often have the same weight as edge j, i and the adjacency matrix will be symmetric.\n",
    "\n",
    "Many convolutional and pooling layers in Spektral use this sparse representation of matrices to do their computation, and sometimes you will see in the documentation a comment saying that \"This layer expects a sparse adjacency matrix.\"\n",
    "\n",
    "### Node features (graph.x)\n",
    "\n",
    "When working with graph neural networks, we usually associate a vector of features with each node of a graph. This is no different from how every pixel in an image has an [R, G, B, A] vector associated with it.\n",
    "\n",
    "Since we have n_nodes nodes and each node has a feature vector of size n_node_features, we can stack all features in a matrix x of shape [n_nodes, n_node_features].\n",
    "\n",
    "In Spektral, x is always represented with a dense np.array (since in this case we don't run the risk of storing many useless zeros -- at least not often).\n",
    "\n",
    "\n",
    "### Edge features (graph.e)\n",
    "Similar to node features, we can also have features associated with edges. These are usually different from the edge weights that we saw for the adjacency matrix, and often represent the kind of relation between two nodes (e.g., acquaintances, friends, or partners).\n",
    "\n",
    "When representing edge features, we run into the same problems that we have for the adjacency matrix.\n",
    "\n",
    "If we store them in a dense np.array, then the array will have shape [n_nodes, n_nodes, n_edge_features] and most of its entries will be zeros. Unfortunately, order-3 tensors cannot be represented as Scipy sparse matrices, so we need to be smart about it.\n",
    "\n",
    "Similar to how we stored the adjacency matrix as a list of entries r, c, v, here we can use the COOrdinate format to represent our edge features. Assume that, in the example above, each edge has n_edge_features=3 features. We could do something like:\n",
    "\n",
    "                R, C, V\n",
    "            0, 1, [ef_1, ef_2, ef_3]\n",
    "            0, 3, [ef_1, ef_2, ef_3]\n",
    "            1, 0, [ef_1, ef_2, ef_3]\n",
    "            1, 2, [ef_1, ef_2, ef_3]\n",
    "            2, 1, [ef_1, ef_2, ef_3]\n",
    "            2, 3, [ef_1, ef_2, ef_3]\n",
    "            3, 0, [ef_1, ef_2, ef_3]\n",
    "            3, 2, [ef_1, ef_2, ef_3]\n",
    "            \n",
    "            \n",
    "            \n",
    "Since we already have the information of R and C in the adjacency matrix, we only need to store the V column as a matrix e of shape [n_edges, n_edge_features]. In this case, n_edges indicates the number of non-zero entries in the adjacency matrix.\n",
    "\n",
    "Note that, since we have separated the edge features from the edge indices of the adjacency matrix, the order in which we store the edge features is very important. We must not break the correspondence between the edges in a and the edges in e.\n",
    "\n",
    "In Spektral, we always assume that edges are sorted in the row-major ordering (we first sort by row, then by column, like in the example above). This is not important when building the adjacency matrix, but it is important when building e.\n",
    "\n",
    "You can use spektral.utils.sparse.reorder to sort a matrix of edge features in the correct row-major order given by an edge index (i.e., the matrix obtained by stacking the R and C columns).\n",
    "\n",
    "\n",
    "### Labels (graph.y)\n",
    "Finally, in many machine learning tasks we want to predict a label given an input. When working with GNNs, labels can be of two types:\n",
    "\n",
    "Graph labels represent some global properties of an entire graph;\n",
    "Node labels represent some properties of each individual node in a graph;\n",
    "Spektral supports both kinds.\n",
    "\n",
    "Labels are dense np.arrays or scalars, stored in the y attribute of a Graph object.\n",
    "Graph-level labels can be either scalars or 1-dimensional arrays of shape [n_labels, ].\n",
    "Node-level labels can be 1-dimensional arrays of shape [n_nodes, ] (representing a scalar label for each node), or 2-dimensional arrays of shape [n_nodes, n_labels].\n",
    "\n",
    "This difference is relevant only when using a DisjointLoader\n",
    "\n",
    "\n",
    "\n",
    "### Datasets\n",
    "The spektral.data.Dataset container provides some useful functionality to manipulate collections of graphs.\n",
    "\n",
    "Let's load a popular benchmark dataset for graph classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spektral in c:\\users\\asa279\\anaconda3\\lib\\site-packages (1.0.7)\n",
      "Requirement already satisfied: requests in c:\\users\\asa279\\anaconda3\\lib\\site-packages (from spektral) (2.25.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\asa279\\anaconda3\\lib\\site-packages (from spektral) (0.24.1)\n",
      "Requirement already satisfied: numpy<1.20 in c:\\users\\asa279\\anaconda3\\lib\\site-packages (from spektral) (1.19.5)\n",
      "Requirement already satisfied: tensorflow>=2.1.0 in c:\\users\\asa279\\anaconda3\\lib\\site-packages (from spektral) (2.5.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\asa279\\anaconda3\\lib\\site-packages (from spektral) (1.2.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\asa279\\anaconda3\\lib\\site-packages (from spektral) (1.0.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\asa279\\anaconda3\\lib\\site-packages (from spektral) (1.6.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\asa279\\anaconda3\\lib\\site-packages (from spektral) (2.5)\n",
      "Requirement already satisfied: lxml in c:\\users\\asa279\\anaconda3\\lib\\site-packages (from spektral) (4.6.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\asa279\\anaconda3\\lib\\site-packages (from spektral) (4.59.0)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in c:\\users\\asa279\\anaconda3\\lib\\site-packages (from tensorflow>=2.1.0->spektral) (1.12)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in c:\\users\\asa279\\anaconda3\\lib\\site-packages (from tensorflow>=2.1.0->spektral) (3.7.4.3)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in c:\\users\\asa279\\anaconda3\\lib\\site-packages (from tensorflow>=2.1.0->spektral) (1.1.2)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in c:\\users\\asa279\\anaconda3\\lib\\site-packages (from tensorflow>=2.1.0->spektral) (1.12.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.6.0,>=2.5.0rc0 in c:\\users\\asa279\\anaconda3\\lib\\site-packages (from tensorflow>=2.1.0->spektral) (2.5.0)\n",
      "Requirement already satisfied: keras-nightly~=2.5.0.dev in c:\\users\\asa279\\anaconda3\\lib\\site-packages (from tensorflow>=2.1.0->spektral) (2.5.0.dev2021032900)\n",
      "Requirement already satisfied: gast==0.4.0 in c:\\users\\asa279\\anaconda3\\lib\\site-packages (from tensorflow>=2.1.0->spektral) (0.4.0)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in c:\\users\\asa279\\anaconda3\\lib\\site-packages (from tensorflow>=2.1.0->spektral) (1.6.3)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in c:\\users\\asa279\\anaconda3\\lib\\site-packages (from tensorflow>=2.1.0->spektral) (3.17.3)\n",
      "Requirement already satisfied: grpcio~=1.34.0 in c:\\users\\asa279\\anaconda3\\lib\\site-packages (from tensorflow>=2.1.0->spektral) (1.34.1)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in c:\\users\\asa279\\anaconda3\\lib\\site-packages (from tensorflow>=2.1.0->spektral) (1.1.0)\n",
      "Requirement already satisfied: wheel~=0.35 in c:\\users\\asa279\\anaconda3\\lib\\site-packages (from tensorflow>=2.1.0->spektral) (0.36.2)\n",
      "Requirement already satisfied: six~=1.15.0 in c:\\users\\asa279\\anaconda3\\lib\\site-packages (from tensorflow>=2.1.0->spektral) (1.15.0)\n",
      "Requirement already satisfied: google-pasta~=0.2 in c:\\users\\asa279\\anaconda3\\lib\\site-packages (from tensorflow>=2.1.0->spektral) (0.2.0)\n",
      "Requirement already satisfied: absl-py~=0.10 in c:\\users\\asa279\\anaconda3\\lib\\site-packages (from tensorflow>=2.1.0->spektral) (0.12.0)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in c:\\users\\asa279\\anaconda3\\lib\\site-packages (from tensorflow>=2.1.0->spektral) (3.3.0)\n",
      "Requirement already satisfied: h5py~=3.1.0 in c:\\users\\asa279\\anaconda3\\lib\\site-packages (from tensorflow>=2.1.0->spektral) (3.1.0)\n",
      "Requirement already satisfied: tensorboard~=2.5 in c:\\users\\asa279\\anaconda3\\lib\\site-packages (from tensorflow>=2.1.0->spektral) (2.5.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\asa279\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow>=2.1.0->spektral) (1.8.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\asa279\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow>=2.1.0->spektral) (0.6.1)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in c:\\users\\asa279\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow>=2.1.0->spektral) (1.33.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\asa279\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow>=2.1.0->spektral) (52.0.0.post20210125)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\asa279\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow>=2.1.0->spektral) (3.3.4)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\asa279\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow>=2.1.0->spektral) (0.4.4)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\asa279\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow>=2.1.0->spektral) (1.0.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\asa279\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow>=2.1.0->spektral) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\users\\asa279\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow>=2.1.0->spektral) (4.2.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\asa279\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow>=2.1.0->spektral) (4.7.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\asa279\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow>=2.1.0->spektral) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\asa279\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow>=2.1.0->spektral) (0.4.8)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\asa279\\anaconda3\\lib\\site-packages (from requests->spektral) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\asa279\\anaconda3\\lib\\site-packages (from requests->spektral) (1.26.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asa279\\anaconda3\\lib\\site-packages (from requests->spektral) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\asa279\\anaconda3\\lib\\site-packages (from requests->spektral) (2.10)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\asa279\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow>=2.1.0->spektral) (3.1.1)\n",
      "Requirement already satisfied: decorator>=4.3.0 in c:\\users\\asa279\\anaconda3\\lib\\site-packages (from networkx->spektral) (5.0.6)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\asa279\\anaconda3\\lib\\site-packages (from pandas->spektral) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\asa279\\anaconda3\\lib\\site-packages (from pandas->spektral) (2021.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\asa279\\anaconda3\\lib\\site-packages (from scikit-learn->spektral) (2.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install spektral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded PROTEINS.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TUDataset(n_graphs=1113)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spektral.datasets import TUDataset\n",
    "\n",
    "dataset = TUDataset('PROTEINS')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Graph(n_nodes=42, n_node_features=4, n_edge_features=None, n_labels=2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can now retrieve individual graphs:\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#or shuffle the data:\n",
    "import numpy as np \n",
    "\n",
    "np.random.shuffle(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'spektral.datasets.tudataset.TUDataset'>\n"
     ]
    }
   ],
   "source": [
    "print(type(dataset))\n",
    "#but built on top of numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TUDataset(n_graphs=100)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#or slice the dataset into sub-datsets:\n",
    "dataset[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasets also provide methods for applying transforms to each datum:\n",
    "\n",
    "- apply(transform) - modifies the dataset in-place, by applying the transform to each graph;\n",
    "- map(transform) - returns a list obtained by applying the transform to each graph;\n",
    "- filter(function) - removes from the dataset any graph for which function(graph) is False. This is also an in-place operation.\n",
    "\n",
    "For example, let's modify our dataset so that we only have graphs with less than 500 nodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TUDataset(n_graphs=1111)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.filter(lambda g: g.n_nodes < 500)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's apply some transforms to our graphs. For example, we can modify each graph so that the node features also contain the one-hot-encoded degree of the nodes.\n",
    "\n",
    "First, we compute the maximum degree of the dataset, so that we know the size of the one-hot vectors: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_degree = dataset.map(lambda g: g.a.sum(-1).max(), reduce=max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to go over the lambda function to see what it does. Also, notice that we passed a reduction function to the method, using the reduce keyword. This will be run on the output list computed by the map.\n",
    "\n",
    "Now we are ready to augment our node features with the one-hot-encoded degree. Spektral has a lot of pre-implemented transforms that we can use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spektral.transforms import Degree\n",
    "dataset.apply(Degree(int(max_degree)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that it worked because now we have an extra max_degree + 1 node features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Graph(n_nodes=23, n_node_features=56, n_edge_features=None, n_labels=2)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we will be using a GCNConv layer in our GNN, we also want to follow the original paper that introduced this layer and do some extra pre-processing of the adjacency matrix.\n",
    "\n",
    "Since this is a fairly common operation, Spektral has a transform to do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spektral.transforms import GCNFilter\n",
    "dataset.apply(GCNFilter())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many layers will require you to do some form of preprocessing. If you don't want to go back to the literature every time, every convolutional layer in Spektral has a preprocess(a) method that you can use to transform the adjacency matrix as needed.\n",
    "\n",
    "Have a look at the handy LayerPreprocess transform.\n",
    "\n",
    "## Creating a GNN\n",
    "\n",
    "Creating GNNs is where Spektral really shines. Since Spektral is designed as an extension of Keras, you can plug any Spektral layer into a Keras Model without modifications.\n",
    "We just need to use the functional API because GNN layers usually need two or more inputs (so no Sequential models for now).\n",
    "\n",
    "For our first GNN, we will create a simple network that first does a bit of graph convolution, then sums all the nodes together (known as \"global pooling\"), and finally classifies the result with a dense softmax layer. We will also use dropout for regularization.\n",
    "\n",
    "Let's start by importing the necessary layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from spektral.layers import GCNConv, GlobalSumPool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use model subclassing to define our model:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyFirstGNN(Model):\n",
    "\n",
    "    def __init__(self, n_hidden, n_labels):\n",
    "        super().__init__()\n",
    "        self.graph_conv = GCNConv(n_hidden)\n",
    "        self.pool = GlobalSumPool()\n",
    "        self.dropout = Dropout(0.5)\n",
    "        self.dense = Dense(n_labels, 'softmax')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        out = self.graph_conv(inputs)\n",
    "        out = self.dropout(out)\n",
    "        out = self.pool(out)\n",
    "        out = self.dense(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it!\n",
    "\n",
    "Note how we mixed layers from Spektral and Keras interchangeably: it's all just computation with tensors underneath.\n",
    "\n",
    "This also means that if you want to break free from Graph and Dataset and every other feature of Spektral, you can.\n",
    "\n",
    "Note: If you don't want to subclass Model to implement your GNN, you can also use the classical declarative style. You just need to pay attention to the Input and leave \"node\" dimensions unspecified (so None instead of n_nodes).\n",
    "\n",
    "### Training the GNN\n",
    "Now we're ready to train the GNN. First, we instantiate and compile our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyFirstGNN(32, dataset.n_labels)\n",
    "model.compile('adam', 'categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nd we're almost there!\n",
    "\n",
    "However, here's where graphs get in our way. Unlike regular data, like images or sequences, graphs cannot be stretched, cut, or reshaped so that we can fit them into tensors of pre-defined shapes. If a graph has 10 nodes and another one has 4, we have to keep them that way.\n",
    "\n",
    "This means that iterating over a dataset in mini-batches is not trivial and we cannot simply use the model.fit() method of Keras as-is.\n",
    "\n",
    "We have to use a data Loader."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loaders\n",
    "\n",
    "Loaders iterate over a graph dataset to create mini-batches. They hide a lot of the complexity behind the process so that you don't need to think about it. You only need to go to this page and read up on data modes, so that you know which loader to use.\n",
    "\n",
    "Each loader has a load() method that returns a data generator that Keras can process.\n",
    "\n",
    "Since we're doing graph-level classification, we can use a BatchLoader. It's a bit slow and memory intensive (a DisjointLoader would have been better), but it lets us simplify the definition of MyFirstGNN. Again, go read about data modes after this tutorial.\n",
    "\n",
    "Let's create a data loader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spektral.data import BatchLoader\n",
    "\n",
    "loader = BatchLoader(dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and we can finally train our GNN!\n",
    "\n",
    "Since loaders are essentially generators, we need to provide the steps_per_epoch keyword to model.fit() and we don't need to specify a batch size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "35/35 [==============================] - 2s 10ms/step - loss: 30.8244\n",
      "Epoch 2/10\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 5.5156\n",
      "Epoch 3/10\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 3.8504\n",
      "Epoch 4/10\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 3.6284\n",
      "Epoch 5/10\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 3.1470\n",
      "Epoch 6/10\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 2.7832\n",
      "Epoch 7/10\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 2.6813\n",
      "Epoch 8/10\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 2.7632\n",
      "Epoch 9/10\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 2.6673\n",
      "Epoch 10/10\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 2.9018\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a9288b4d60>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(loader.load(), steps_per_epoch=loader.steps_per_epoch, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node-level learning\n",
    "Besides learning to predict labels for the whole graph, like in this tutorial, GNNs are very effective at learning to predict labels for each node. This is called \"node-level learning\" and we usually do it for datasets with one big graph (think a social network)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Practices\n",
    "\n",
    "## Going beyond the Sequential model: the Keras functional API\n",
    "\n",
    "The Keras functional API is a way to create models that are more flexible than the tf.keras.Sequential API. The functional API can handle models with non-linear topology, shared layers, and even multiple inputs or outputs.\n",
    "\n",
    "The main idea is that a deep learning model is usually a directed acyclic graph (DAG) of layers. So the functional API is a way to build graphs of layers.\n",
    "Consider the following model:\n",
    "\n",
    "```\n",
    "(input: 784-dimensional vectors)\n",
    "       ↧\n",
    "[Dense (64 units, relu activation)]\n",
    "       ↧\n",
    "[Dense (64 units, relu activation)]\n",
    "       ↧\n",
    "[Dense (10 units, softmax activation)]\n",
    "       ↧\n",
    "(output: logits of a probability distribution over 10 classes)\n",
    "```\n",
    "\n",
    "This is a basic graph with three layers.\n",
    "Let's see how we can create this type of models on this link:https://colab.research.google.com/github/keras-team/keras-io/blob/master/guides/ipynb/functional_api.ipynb#scrollTo=6Q1lukiNDLoy\n",
    "\n",
    "\n",
    "## Inspecting and monitoring deep-learning models using Keras callbacks and TensorBoard\n",
    "\n",
    "\n",
    "https://keras.io/guides/writing_your_own_callbacks/\n",
    "\n",
    "\n",
    "## Hyperparameter optimization\n",
    "\n",
    "#### \n",
    "1. Choose a set of hyperparameters (automatically).\n",
    "2. Build the corresponding model.\n",
    "3. Fit it to your training data, and measure the final performance on the validation\n",
    "data.\n",
    "4. Choose the next set of hyperparameters to try (automatically).\n",
    "5. Repeat.\n",
    "6. Eventually, measure performance on your test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install keras-tuner -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "def build_model(hp):\n",
    "    units = hp.Int(name=\"units\", min_value=16, max_value=64, step=16)\n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(units, activation=\"relu\"),\n",
    "        layers.Dense(10, activation=\"softmax\")\n",
    "    ])\n",
    "    optimizer = hp.Choice(name=\"optimizer\", values=[\"rmsprop\", \"adam\"])\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-34-643dcc05e516>:1: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n",
      "  import kerastuner as kt\n"
     ]
    }
   ],
   "source": [
    "import kerastuner as kt\n",
    "\n",
    "class SimpleMLP(kt.HyperModel):\n",
    "    def __init__(self, num_classes):\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def build(self, hp):\n",
    "        units = hp.Int(name=\"units\", min_value=16, max_value=64, step=16)\n",
    "        model = keras.Sequential([\n",
    "            layers.Dense(units, activation=\"relu\"),\n",
    "            layers.Dense(self.num_classes, activation=\"softmax\")\n",
    "        ])\n",
    "        optimizer = hp.Choice(name=\"optimizer\", values=[\"rmsprop\", \"adam\"])\n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss=\"sparse_categorical_crossentropy\",\n",
    "            metrics=[\"accuracy\"])\n",
    "        return model\n",
    "\n",
    "hypermodel = SimpleMLP(num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tuner = kt.BayesianOptimization(\n",
    "    build_model,\n",
    "    objective=\"val_accuracy\",\n",
    "    max_trials=100,\n",
    "    executions_per_trial=2,\n",
    "    directory=\"mnist_kt_test\",\n",
    "    overwrite=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search space summary\n",
      "Default search space size: 2\n",
      "units (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 16, 'max_value': 64, 'step': 16, 'sampling': None}\n",
      "optimizer (Choice)\n",
      "{'default': 'rmsprop', 'conditions': [], 'values': ['rmsprop', 'adam'], 'ordered': False}\n"
     ]
    }
   ],
   "source": [
    "tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5 Complete [00h 00m 05s]\n",
      "val_accuracy: 0.9662500023841858\n",
      "\n",
      "Best val_accuracy So Far: 0.9662500023841858\n",
      "Total elapsed time: 00h 00m 25s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "x_train = x_train.reshape((-1, 28 * 28)).astype(\"float32\") / 255\n",
    "x_test = x_test.reshape((-1, 28 * 28)).astype(\"float32\") / 255\n",
    "x_train_full = x_train[:]\n",
    "y_train_full = y_train[:]\n",
    "num_val_samples = 10000\n",
    "x_train, x_val = x_train[:-num_val_samples], x_train[-num_val_samples:]\n",
    "y_train, y_val = y_train[:-num_val_samples], y_train[-num_val_samples:]\n",
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5),\n",
    "]\n",
    "tuner.search(\n",
    "    x_train, y_train,\n",
    "    batch_size=128,\n",
    "    epochs=5,\n",
    "    validation_data=(x_val, y_val),\n",
    "    callbacks=callbacks,\n",
    "    verbose=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Querying the best hyperparameter configurations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n = 4\n",
    "best_hps = tuner.get_best_hyperparameters(top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_best_epoch(hp):\n",
    "    model = build_model(hp)\n",
    "    callbacks=[\n",
    "        keras.callbacks.EarlyStopping(\n",
    "            monitor=\"val_loss\", mode=\"min\", patience=10)\n",
    "    ]\n",
    "    history = model.fit(\n",
    "        x_train, y_train,\n",
    "        validation_data=(x_val, y_val),\n",
    "        epochs=5,\n",
    "        batch_size=128,\n",
    "        callbacks=callbacks)\n",
    "    val_loss_per_epoch = history.history[\"val_loss\"]\n",
    "    best_epoch = val_loss_per_epoch.index(min(val_loss_per_epoch)) + 1\n",
    "    print(f\"Best epoch: {best_epoch}\")\n",
    "    return best_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "391/391 [==============================] - 1s 1ms/step - loss: 0.4246 - accuracy: 0.8855 - val_loss: 0.2248 - val_accuracy: 0.9378\n",
      "Epoch 2/5\n",
      "391/391 [==============================] - 0s 1ms/step - loss: 0.2171 - accuracy: 0.9378 - val_loss: 0.1772 - val_accuracy: 0.9504\n",
      "Epoch 3/5\n",
      "391/391 [==============================] - 0s 875us/step - loss: 0.1726 - accuracy: 0.9499 - val_loss: 0.1519 - val_accuracy: 0.9577\n",
      "Epoch 4/5\n",
      "391/391 [==============================] - 0s 951us/step - loss: 0.1428 - accuracy: 0.9583 - val_loss: 0.1358 - val_accuracy: 0.9623\n",
      "Epoch 5/5\n",
      "391/391 [==============================] - 0s 1ms/step - loss: 0.1201 - accuracy: 0.9650 - val_loss: 0.1232 - val_accuracy: 0.9642\n",
      "Best epoch: 5\n",
      "Epoch 1/6\n"
     ]
    },
    {
     "ename": "OperatorNotAllowedInGraphError",
     "evalue": "in user code:\n\n    C:\\Users\\asa279\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:855 train_function  *\n        return step_function(self, iterator)\n    <ipython-input-21-bc75defdf961>:11 call  *\n        out = self.graph_conv(inputs)\n    C:\\Users\\asa279\\Anaconda3\\lib\\site-packages\\spektral\\layers\\convolutional\\conv.py:99 _inner_check_dtypes  *\n        inputs = check_dtypes(inputs)\n    C:\\Users\\asa279\\Anaconda3\\lib\\site-packages\\spektral\\layers\\convolutional\\conv.py:81 check_dtypes  *\n        x, a = inputs\n    C:\\Users\\asa279\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:520 __iter__\n        self._disallow_iteration()\n    C:\\Users\\asa279\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:513 _disallow_iteration\n        self._disallow_when_autograph_enabled(\"iterating over `tf.Tensor`\")\n    C:\\Users\\asa279\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:489 _disallow_when_autograph_enabled\n        raise errors.OperatorNotAllowedInGraphError(\n\n    OperatorNotAllowedInGraphError: iterating over `tf.Tensor` is not allowed: AutoGraph did convert this function. This might indicate you are trying to use an unsupported feature.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOperatorNotAllowedInGraphError\u001b[0m            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-49-61a42ae1653c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mbest_models\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mhp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbest_hps\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_best_trained_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mbest_models\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbest_models\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbest_models\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-49-61a42ae1653c>\u001b[0m in \u001b[0;36mget_best_trained_model\u001b[1;34m(hp)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_best_trained_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mbest_epoch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_best_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     model.fit(\n\u001b[0m\u001b[0;32m      4\u001b[0m         \u001b[0mx_train_full\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_full\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         batch_size=128, epochs=int(best_epoch * 1.2))\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1181\u001b[0m                 _r=1):\n\u001b[0;32m   1182\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1183\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1184\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1185\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    887\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    915\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 917\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    918\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3020\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3021\u001b[0m       (graph_function,\n\u001b[1;32m-> 3022\u001b[1;33m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0m\u001b[0;32m   3023\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m   3024\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_signature\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3439\u001b[0m               call_context_key in self._function_cache.missed):\n\u001b[1;32m-> 3440\u001b[1;33m             return self._define_function_with_shape_relaxation(\n\u001b[0m\u001b[0;32m   3441\u001b[0m                 args, kwargs, flat_args, filtered_flat_args, cache_key_context)\n\u001b[0;32m   3442\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_define_function_with_shape_relaxation\u001b[1;34m(self, args, kwargs, flat_args, filtered_flat_args, cache_key_context)\u001b[0m\n\u001b[0;32m   3360\u001b[0m           expand_composites=True)\n\u001b[0;32m   3361\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3362\u001b[1;33m     graph_function = self._create_graph_function(\n\u001b[0m\u001b[0;32m   3363\u001b[0m         args, kwargs, override_flat_arg_shapes=relaxed_arg_shapes)\n\u001b[0;32m   3364\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marg_relaxed\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrank_only_cache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3277\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3278\u001b[0m     graph_function = ConcreteFunction(\n\u001b[1;32m-> 3279\u001b[1;33m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[0;32m   3280\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3281\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    997\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    998\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 999\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1000\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1001\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    670\u001b[0m         \u001b[1;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    671\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcompile_with_xla\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 672\u001b[1;33m           \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    673\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    674\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    984\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    985\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 986\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    987\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    988\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOperatorNotAllowedInGraphError\u001b[0m: in user code:\n\n    C:\\Users\\asa279\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:855 train_function  *\n        return step_function(self, iterator)\n    <ipython-input-21-bc75defdf961>:11 call  *\n        out = self.graph_conv(inputs)\n    C:\\Users\\asa279\\Anaconda3\\lib\\site-packages\\spektral\\layers\\convolutional\\conv.py:99 _inner_check_dtypes  *\n        inputs = check_dtypes(inputs)\n    C:\\Users\\asa279\\Anaconda3\\lib\\site-packages\\spektral\\layers\\convolutional\\conv.py:81 check_dtypes  *\n        x, a = inputs\n    C:\\Users\\asa279\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:520 __iter__\n        self._disallow_iteration()\n    C:\\Users\\asa279\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:513 _disallow_iteration\n        self._disallow_when_autograph_enabled(\"iterating over `tf.Tensor`\")\n    C:\\Users\\asa279\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:489 _disallow_when_autograph_enabled\n        raise errors.OperatorNotAllowedInGraphError(\n\n    OperatorNotAllowedInGraphError: iterating over `tf.Tensor` is not allowed: AutoGraph did convert this function. This might indicate you are trying to use an unsupported feature.\n"
     ]
    }
   ],
   "source": [
    "def get_best_trained_model(hp):\n",
    "    best_epoch = get_best_epoch(hp)\n",
    "    model.fit(\n",
    "        x_train_full, y_train_full,\n",
    "        batch_size=128, epochs=int(best_epoch * 1.2))\n",
    "    return model\n",
    "\n",
    "best_models = []\n",
    "for hp in best_hps:\n",
    "    model = get_best_trained_model(hp)\n",
    "    model.evaluate(x_test, y_test)\n",
    "    best_models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_models = tuner.get_best_models(top_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model ensembling\n",
    "\"Another powerful technique for obtaining the best possible results on a task is model\n",
    "ensembling. Ensembling consists of pooling together the predictions of a set of different\n",
    "models, to produce better predictions. If you look at machine-learning competitions,\n",
    "in particular on Kaggle, you’ll see that the winners use very large ensembles of\n",
    "models that inevitably beat any single model, no matter how good.\"\n",
    "\n",
    "\n",
    "    preds_a = model_a.predict(x_val)\n",
    "    preds_b = model_b.predict(x_val)\n",
    "    preds_c = model_c.predict(x_val)\n",
    "    preds_d = model_d.predict(x_val)\n",
    "    final_preds = 0.25 * (preds_a + preds_b + preds_c + preds_d)\n",
    "    \n",
    " \n",
    " \n",
    " A smarter way to ensemble classifiers is to do a weighted average, where the\n",
    "weights are learned on the validation data—typically, the better classifiers are given a\n",
    "higher weight, and the worse classifiers are given a lower weight. To search for a good\n",
    "set of ensembling weights, you can use random search or a simple optimization algorithm\n",
    "such as Nelder-Mead:\n",
    "\n",
    "    preds_a = model_a.predict(x_val)\n",
    "    preds_b = model_b.predict(x_val)\n",
    "    preds_c = model_c.predict(x_val)\n",
    "    preds_d = model_d.predict(x_val)\n",
    "    final_preds = 0.5 * preds_a + 0.25 * preds_b + 0.1 * preds_c + 0.15 * preds_d\n",
    "    \n",
    "Winning machine-learning competitions or otherwise obtaining the best possible\n",
    "results on a task can only be done with large ensembles of models. Ensembling\n",
    "via a well-optimized weighted average is usually good enough. Remember:\n",
    "diversity is strength."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-GPU and distributed training (optional)\n",
    "There are generally two ways to distribute computation across multiple devices:\n",
    "\n",
    "**Data parallelism**, where a single model gets replicated on multiple devices or\n",
    "multiple machines. Each of them processes different batches of data, then they merge\n",
    "their results. There exist many variants of this setup, that differ in how the different\n",
    "model replicas merge results, in whether they stay in sync at every batch or whether they\n",
    "are more loosely coupled, etc.\n",
    "\n",
    "**Model parallelism**, where different parts of a single model run on different devices,\n",
    "processing a single batch of data together. This works best with models that have a\n",
    "naturally-parallel architecture, such as models that feature multiple branches.\n",
    "\n",
    "This guide focuses on data parallelism, in particular **synchronous data parallelism**,\n",
    "where the different replicas of the model stay in sync after each batch they process.\n",
    "Synchronicity keeps the model convergence behavior identical to what you would see for\n",
    "single-device training.\n",
    "\n",
    "Specifically, this guide teaches you how to use the `tf.distribute` API to train Keras\n",
    "models on multiple GPUs, with minimal changes to your code, in the following two setups:\n",
    "\n",
    "- On multiple GPUs (typically 2 to 8) installed on a single machine (single host,\n",
    "multi-device training). This is the most common setup for researchers and small-scale\n",
    "industry workflows.\n",
    "- On a cluster of many machines, each hosting one or multiple GPUs (multi-worker\n",
    "distributed training). This is a good setup for large-scale industry workflows, e.g.\n",
    "training high-resolution image classification models on tens of millions of images using\n",
    "20-100 GPUs.\n",
    "\n",
    "\n",
    "https://colab.research.google.com/github/keras-team/keras-io/blob/master/guides/ipynb/distributed_training.ipynb\n",
    "\n",
    "## Training Keras models with TensorFlow Cloud (optional)\n",
    "TensorFlow Cloud is a library that makes it easier to do training and hyperparameter tuning of Keras models on Google Cloud.\n",
    "\n",
    "Using TensorFlow Cloud's run API, you can send your model code directly to your Google Cloud account, and use Google Cloud compute resources without needing to login and interact with the Cloud UI (once you have set up your project in the console).\n",
    "\n",
    "This means that you can use your Google Cloud compute resources from inside directly a Python notebook: a notebook just like this one! You can also send models to Google Cloud from a plain .py Python script.\n",
    "\n",
    "https://colab.research.google.com/github/keras-team/keras-io/blob/master/guides/ipynb/training_keras_models_on_cloud.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "- https://www.tensorflow.org/tutorials\n",
    "- keras.io/guides/\n",
    "- Deep Learning with Python"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
